name: Kafka GitOps CI flow
on:
  pull_request:
    branches: [ main, features ]
    types: [opened, synchronize, reopened]
    paths:
      - 'domains/**/kafka-request.yaml'
      - 'domains/**/schemas/**'

jobs:
  detect:
    runs-on: ubuntu-latest
    outputs:
      env: ${{ steps.pick.outputs.env }}
      file: ${{ steps.pick.outputs.file }}
      domain: ${{ steps.pick.outputs.domain }}
      matrix: ${{ steps.pick.outputs.matrix }}
    steps:
      - uses: actions/checkout@v4
      - name: Detect environment(s) from changed kafka-request.yaml files
        id: pick
        run: |
          # Find all changed kafka-request.yaml files
          files=$(git ls-files 'domains/**/kafka-request.yaml')
          
          if [ -z "$files" ]; then
            echo "No kafka-request.yaml found in domains/" >&2
            exit 1
          fi
          
          # Extract unique environments
          envs=$(echo "$files" | cut -d'/' -f3 | sort -u)
          
          if [ -z "$envs" ]; then
            echo "Error: Could not extract environment from path" >&2
            exit 1
          fi
          
          # Use the first environment found (in this serial approach)
          env_name=$(echo "$envs" | head -n1)
          first_file=$(echo "$files" | head -n1)
          domain_name=$(echo "$first_file" | cut -d'/' -f2)
          
          echo "Detected environment: $env_name"
          echo "Detected domain: $domain_name"
          echo "Changed files: $files"
          
          {
            echo "env=$env_name"
            echo "file=$first_file"
            echo "domain=$domain_name"
            echo "matrix=$(echo \"$files\" | jq -R -s -c 'split("\n")[:-1] | map({file:.})' | tr -d ' ')"
          } >> "$GITHUB_OUTPUT"

  plan:
    needs: [detect]
    environment: ${{ needs.detect.outputs.env }}
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: ${{ needs.detect.outputs.env }}
      DOMAIN: ${{ needs.detect.outputs.domain }}
      TF_VAR_confluent_api_key: ${{ secrets.CONFLUENT_API_KEY }}
      TF_VAR_confluent_api_secret: ${{ secrets.CONFLUENT_API_SECRET }}
      TF_VAR_kafka_api_key: ${{ secrets.KAFKA_API_KEY }}
      TF_VAR_kafka_api_secret: ${{ secrets.KAFKA_API_SECRET }}
      ORGANIZATION_ID: ${{ vars.ORGANIZATION_ID }}
      ENVIRONMENT_ID: ${{ vars.ENVIRONMENT_ID }}
      KAFKA_CLUSTER_ID: ${{ vars.KAFKA_CLUSTER_ID }}
      REST_ENDPOINT: ${{ vars.REST_ENDPOINT }}
      SCHEMA_REGISTRY_ID: ${{ vars.SCHEMA_REGISTRY_ID }}
      SCHEMA_REGISTRY_API_KEY: ${{ secrets.SCHEMA_REGISTRY_API_KEY }}
      SCHEMA_REGISTRY_API_SECRET: ${{ secrets.SCHEMA_REGISTRY_API_SECRET }}
      SCHEMA_REGISTRY_REST_ENDPOINT: ${{ vars.SCHEMA_REGISTRY_REST_ENDPOINT }}

    steps:
      - uses: actions/checkout@v4
      - uses: hashicorp/setup-terraform@v3

      - name: Validate AWS settings exist
        run: |
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ] || [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
            echo "Error: Missing AWS secrets AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY."
            echo "Define repository-level secrets under Settings → Secrets and variables → Actions."
            exit 1
          fi
          if [ -z "${{ vars.AWS_REGION }}" ]; then
            echo "Error: Missing repository variable AWS_REGION."
            echo "Define repository-level variable AWS_REGION under Settings → Secrets and variables → Actions."
            exit 1
          fi
          if [ -z "${{ vars.TF_BUCKET_STATE }}" ]; then
            echo "Error: Missing repository variable TF_BUCKET_STATE."
            echo "Define repository-level variable TF_BUCKET_STATE under Settings → Secrets and variables → Actions."
            exit 1
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          aws-secret-access-key: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          aws-region: "${{ vars.AWS_REGION }}"

      - name: Verify AWS identity
        run: aws sts get-caller-identity

      - name: Export global variables to env
        run: |
          echo "TF_BUCKET_STATE=${{ vars.TF_BUCKET_STATE }}" >> "$GITHUB_ENV"

      - name: Check environment lock (only one deployment at a time)
        run: |
          LOCK_FILE="catalogs/${{ env.ENVIRONMENT }}/.lock"
          if [ -f "$LOCK_FILE" ]; then
            echo "❌ Environment ${{ env.ENVIRONMENT }} is locked"
            echo ""
            echo "Another developer is currently deploying to this environment."
            echo "Please wait for their deployment to complete before retrying this PR."
            echo ""
            echo "Lock file: $LOCK_FILE"
            exit 1
          fi
          echo "✓ Environment lock check passed"

      - name: Validate API secrets exist
        run: |
          if [ -z "$TF_VAR_confluent_api_key" ] || [ -z "$TF_VAR_confluent_api_secret" ]; then
            echo "Error: Confluent Cloud API secrets missing for environment $ENVIRONMENT."
            echo "Define environment-scoped secrets CONFLUENT_API_KEY and CONFLUENT_API_SECRET under Settings → Environments → $ENVIRONMENT."
            exit 1
          fi
          if [ -z "$TF_VAR_kafka_api_key" ] || [ -z "$TF_VAR_kafka_api_secret" ]; then
            echo "Error: Kafka API secrets missing for environment $ENVIRONMENT."
            echo "Define environment-scoped secrets KAFKA_API_KEY and KAFKA_API_SECRET under Settings → Environments → $ENVIRONMENT."
            exit 1
          fi

      - name: Lint kafka-request.yaml files
        run: |
          pip install yamllint
          for file in domains/*/${{ env.ENVIRONMENT }}/kafka-request.yaml; do
            if [ -f "$file" ]; then
              echo "Linting $file..."
              yamllint "$file"
            fi
          done

      - name: Aggregate kafka resources
        run: |
          pip install pyyaml
          python scripts/aggregate-kafka.py --env ${{ env.ENVIRONMENT }} --output-dir catalogs
          
      - name: Aggregate schemas
        run: |
          python scripts/aggregate-schemas.py --env ${{ env.ENVIRONMENT }} --output-dir catalogs

      - name: Validate cross-domain conflicts
        run: |
          pip install pyyaml
          DEPLOYED_KAFKA="catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml.deployed"
          DEPLOYED_SCHEMAS="catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json.deployed"
          BASE_BRANCH="${{ github.base_ref }}"
          
          # Try to load deployed catalogs from target branch if they exist
          if git show "origin/${BASE_BRANCH}:catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" > "$DEPLOYED_KAFKA" 2>/dev/null; then
            echo "✓ Loaded deployed kafka catalog from ${BASE_BRANCH} for comparison"
          else
            echo "ℹ️  No deployed kafka catalog found on ${BASE_BRANCH} (first deployment for this environment?)"
            rm -f "$DEPLOYED_KAFKA"
          fi
          
          if git show "origin/${BASE_BRANCH}:catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json" > "$DEPLOYED_SCHEMAS" 2>/dev/null; then
            echo "✓ Loaded deployed schemas catalog from ${BASE_BRANCH} for comparison"
          else
            echo "ℹ️  No deployed schemas catalog found on ${BASE_BRANCH} (first deployment for this environment?)"
            rm -f "$DEPLOYED_SCHEMAS"
          fi
          
          python scripts/validate-conflicts.py \
            --env ${{ env.ENVIRONMENT }} \
            --branch-kafka "catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" \
            --branch-schemas "catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json" \
            --deployed-kafka "$DEPLOYED_KAFKA" \
            --deployed-schemas "$DEPLOYED_SCHEMAS"

      - name: Generate tfvars from aggregated catalog
        run: |
          echo "Generating tfvars from aggregated kafka-catalog.yaml..."
          python scripts/parser.py "catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" terraform/terraform.tfvars.json

      - name: Terraform Init
        run: |
          terraform -chdir=terraform init \
            -backend-config="key=terraform/${{ env.DOMAIN }}/${{ env.ENVIRONMENT }}/data-streaming-platform.tfstate" \
            -backend-config="bucket=$TF_BUCKET_STATE" \
            -backend-config="region=$AWS_REGION" \
            -reconfigure

      - name: Terraform validate
        run: terraform -chdir=terraform validate

      - name: Debug:Check generated tfvars
        run: |
          echo "Checking if terraform.tfvars.json exists:"
          ls -la terraform/terraform.tfvars.json
          echo "Content of terraform.tfvars.json:"
          cat terraform/terraform.tfvars.json
          
      - name: Terraform Plan
        run: terraform -chdir=terraform plan -no-color -out=tfplan.bin -input=false -var-file="terraform.tfvars.json" -var="github_repo_owner=${{ github.repository_owner }}" -var="github_repo_name=${{ github.event.repository.name }}" -var="github_token=${{ secrets.GITHUB_TOKEN }}" -var="github_environment=${{ needs.detect.outputs.env }}"
        continue-on-error: true

      - name: Save human-readable plan
        run: terraform -chdir=terraform show -no-color tfplan.bin > terraform/plan.txt
        
      - name: Post plan as PR comment
        uses: actions/github-script@v7
        with:
          github-token: "${{ secrets.GITHUB_TOKEN }}"
          script: |
            const fs = require('fs');
            const plan = fs.readFileSync('terraform/plan.txt', 'utf8');
            const max = 65000;
            const bodyPlan = plan.length > max ? plan.slice(0, max) + '\n\n***plan truncated***' : plan;
            const body = '### Terraform plan\n\n```text\n' + bodyPlan + '\n```';
            const prNumber = context.payload.pull_request.number;
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: prNumber,
              body,
            });