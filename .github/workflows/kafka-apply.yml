name: Kafka GitOps CD flow
on:
  push:
    branches: [ main, features ]
    paths:
      - 'domains/**/kafka-request.yaml'
      - 'domains/**/schemas/**'

jobs:
  detect:
    runs-on: ubuntu-latest
    outputs:
      env: ${{ steps.pick.outputs.env }}
      domain: ${{ steps.pick.outputs.domain }}
    steps:
      - uses: actions/checkout@v4
      - name: Detect environment(s) from changed files
        id: pick
        run: |
          # Find all changed kafka-request.yaml files
          files=$(git ls-files 'domains/**/kafka-request.yaml')
          
          if [ -z "$files" ]; then
            echo "No kafka-request.yaml found in domains/" >&2
            exit 1
          fi
          
          # Extract unique environments
          envs=$(echo "$files" | cut -d'/' -f3 | sort -u)
          
          if [ -z "$envs" ]; then
            echo "Error: Could not extract environment from path" >&2
            exit 1
          fi
          
          # Use the first environment found (in this serial approach)
          env_name=$(echo "$envs" | head -n1)
          domain_name=$(echo "$files" | head -n1 | cut -d'/' -f2)
          
          echo "Detected environment: $env_name"
          echo "Detected domain: $domain_name"
          
          {
            echo "env=$env_name"
            echo "domain=$domain_name"
          } >> "$GITHUB_OUTPUT"

  apply:
    needs: [detect]
    environment: ${{ needs.detect.outputs.env }}
    runs-on: ubuntu-latest
    env:
      ENVIRONMENT: "${{ needs.detect.outputs.env }}"
      DOMAIN: "${{ needs.detect.outputs.domain }}"
      TF_VAR_confluent_api_key: "${{ secrets.CONFLUENT_API_KEY }}"
      TF_VAR_confluent_api_secret: "${{ secrets.CONFLUENT_API_SECRET }}"
      TF_VAR_kafka_api_key: "${{ secrets.KAFKA_API_KEY }}"
      TF_VAR_kafka_api_secret: "${{ secrets.KAFKA_API_SECRET }}"
      ORGANIZATION_ID: "${{ vars.ORGANIZATION_ID }}"
      ENVIRONMENT_ID: "${{ vars.ENVIRONMENT_ID }}"
      KAFKA_CLUSTER_ID: "${{ vars.KAFKA_CLUSTER_ID }}"
      REST_ENDPOINT: "${{ vars.REST_ENDPOINT }}"
      SCHEMA_REGISTRY_ID: "${{ vars.SCHEMA_REGISTRY_ID }}"
      SCHEMA_REGISTRY_API_KEY: "${{ secrets.SCHEMA_REGISTRY_API_KEY }}"
      SCHEMA_REGISTRY_API_SECRET: "${{ secrets.SCHEMA_REGISTRY_API_SECRET }}"
      SCHEMA_REGISTRY_REST_ENDPOINT: "${{ vars.SCHEMA_REGISTRY_REST_ENDPOINT }}"
    steps:
      - uses: actions/checkout@v4
      - uses: hashicorp/setup-terraform@v3

      - name: Validate AWS settings exist
        run: |
          if [ -z "${{ secrets.AWS_ACCESS_KEY_ID }}" ] || [ -z "${{ secrets.AWS_SECRET_ACCESS_KEY }}" ]; then
            echo "Error: Missing AWS secrets AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY."
            echo "Define repository-level secrets under Settings → Secrets and variables → Actions."
            exit 1
          fi
          if [ -z "${{ vars.AWS_REGION }}" ]; then
            echo "Error: Missing repository variable AWS_REGION."
            echo "Define repository-level variable AWS_REGION under Settings → Secrets and variables → Actions."
            exit 1
          fi
          if [ -z "${{ vars.TF_BUCKET_STATE }}" ]; then
            echo "Error: Missing repository variable TF_BUCKET_STATE."
            echo "Define repository-level variable TF_BUCKET_STATE under Settings → Secrets and variables → Actions."
            exit 1
          fi

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5
        with:
          aws-access-key-id: "${{ secrets.AWS_ACCESS_KEY_ID }}"
          aws-secret-access-key: "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          aws-region: "${{ vars.AWS_REGION }}"

      - name: Export global variables to env
        run: |
          echo "TF_BUCKET_STATE=${{ vars.TF_BUCKET_STATE }}" >> "$GITHUB_ENV"

      - name: Acquire environment lock
        run: |
          LOCK_FILE="catalogs/${{ env.ENVIRONMENT }}/.lock"
          mkdir -p "catalogs/${{ env.ENVIRONMENT }}"
          
          # Create lock file
          echo "Lock acquired at $(date -u)" > "$LOCK_FILE"
          git add "$LOCK_FILE"
          git config user.name "kafka-automation[bot]"
          git config user.email "kafka-automation@example.com"
          git commit -m "ci: acquire lock for ${{ env.ENVIRONMENT }} deployment"
          git push origin ${{ github.ref_name }}
          
          echo "✓ Lock acquired for ${{ env.ENVIRONMENT }}"

      - name: Validate API secrets exist
        run: |
          if [ -z "$TF_VAR_confluent_api_key" ] || [ -z "$TF_VAR_confluent_api_secret" ]; then
            echo "Error: Confluent Cloud API secrets missing for environment $ENVIRONMENT."
            echo "Define environment-scoped secrets CONFLUENT_API_KEY and CONFLUENT_API_SECRET under Settings → Environments → $ENVIRONMENT."
            exit 1
          fi
          if [ -z "$TF_VAR_kafka_api_key" ] || [ -z "$TF_VAR_kafka_api_secret" ]; then
            echo "Error: Kafka API secrets missing for environment $ENVIRONMENT."
            echo "Define environment-scoped secrets KAFKA_API_KEY and KAFKA_API_SECRET under Settings → Environments → $ENVIRONMENT."
            exit 1
          fi

      - name: Lint kafka-request.yaml files
        run: |
          pip install yamllint
          for file in domains/*/${{ env.ENVIRONMENT }}/kafka-request.yaml; do
            if [ -f "$file" ]; then
              echo "Linting $file..."
              yamllint "$file"
            fi
          done

      - name: Aggregate kafka resources
        run: |
          pip install pyyaml
          python scripts/aggregate-kafka.py --env ${{ env.ENVIRONMENT }} --output-dir catalogs
          
      - name: Aggregate schemas
        run: |
          python scripts/aggregate-schemas.py --env ${{ env.ENVIRONMENT }} --output-dir catalogs

      - name: Validate cross-domain conflicts
        run: |
          pip install pyyaml
          DEPLOYED_KAFKA="catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml.deployed"
          DEPLOYED_SCHEMAS="catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json.deployed"
          DEPLOY_BRANCH="${{ github.ref_name }}"
          
          # Try to load deployed catalogs from current branch if they exist
          if git show "origin/${DEPLOY_BRANCH}:catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" > "$DEPLOYED_KAFKA" 2>/dev/null; then
            echo "✓ Loaded deployed kafka catalog from ${DEPLOY_BRANCH} for comparison"
          else
            echo "ℹ️  No deployed kafka catalog found on ${DEPLOY_BRANCH} (first deployment for this environment?)"
            rm -f "$DEPLOYED_KAFKA"
          fi
          
          if git show "origin/${DEPLOY_BRANCH}:catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json" > "$DEPLOYED_SCHEMAS" 2>/dev/null; then
            echo "✓ Loaded deployed schemas catalog from ${DEPLOY_BRANCH} for comparison"
          else
            echo "ℹ️  No deployed schemas catalog found on ${DEPLOY_BRANCH} (first deployment for this environment?)"
            rm -f "$DEPLOYED_SCHEMAS"
          fi
          
          python scripts/validate-conflicts.py \
            --env ${{ env.ENVIRONMENT }} \
            --branch-kafka "catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" \
            --branch-schemas "catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json" \
            --deployed-kafka "$DEPLOYED_KAFKA" \
            --deployed-schemas "$DEPLOYED_SCHEMAS"

      - name: Generate tfvars from aggregated catalog
        run: |
          echo "Generating tfvars from aggregated kafka-catalog.yaml..."
          python scripts/parser.py "catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml" terraform/terraform.tfvars.json

      - name: Terraform Init
        run: |
          terraform -chdir=terraform init \
            -backend-config="bucket=$TF_BUCKET_STATE" \
            -backend-config="key=terraform/${{ env.DOMAIN }}/${{ env.ENVIRONMENT }}/data-streaming-platform.tfstate" \
            -backend-config="region=$AWS_REGION" \
            -reconfigure

      - name: Terraform Validate
        run: terraform -chdir=terraform validate

      - name: Terraform Plan
        run: terraform -chdir=terraform plan -no-color -out=tfplan.bin -var-file="terraform.tfvars.json" -var="github_repo_owner=${{ github.repository_owner }}" -var="github_repo_name=${{ github.event.repository.name }}" -var="github_token=${{ secrets.GITHUB_TOKEN }}" -var="github_environment=${{ needs.detect.outputs.env }}"

      - name: Terraform Apply
        run: terraform -chdir=terraform apply -no-color -auto-approve tfplan.bin
        env:
          GH_TOKEN: "${{ secrets.GITHUB_TOKEN }}"

      - name: Update catalogs and release lock
        if: success()
        run: |
          git add catalogs/${{ env.ENVIRONMENT }}/kafka-catalog.yaml catalogs/${{ env.ENVIRONMENT }}/schemas-catalog.json
          rm -f "catalogs/${{ env.ENVIRONMENT }}/.lock"
          git add "catalogs/${{ env.ENVIRONMENT }}/.lock"
          git config user.name "kafka-automation[bot]"
          git config user.email "kafka-automation@example.com"
          git commit -m "chore: update catalogs and release lock for ${{ env.ENVIRONMENT }} (deployment successful)" || echo "No catalog changes to commit"
          git push origin ${{ github.ref_name }}
          echo "✓ Deployment successful; lock released"

      - name: Release lock on failure
        if: failure()
        run: |
          LOCK_FILE="catalogs/${{ env.ENVIRONMENT }}/.lock"
          git fetch origin ${{ github.ref_name }}
          rm -f "$LOCK_FILE"
          git add "$LOCK_FILE"
          git config user.name "kafka-automation[bot]"
          git config user.email "kafka-automation@example.com"
          git commit -m "ci: release lock (deployment failed) for ${{ env.ENVIRONMENT }}" || echo "Lock already released"
          git push origin ${{ github.ref_name }}
          echo "❌ Deployment failed; lock released"